## System Architecture Diagram
![Architecture Diagram](https://github.com/COSC-499-W2025/capstone-project-team-7/blob/33c1fa7c7b7b7819d53f6b5cb04b2e6b6a89a8e2/docs/assets/sysArch.png)

## System Component Diagram
![Component Diagram](https://github.com/COSC-499-W2025/capstone-project-team-7/blob/a51b2ecd844ba9c5b78f8e71f481ff9dec6c95b9/docs/assets/componentDiagram.png)

## System Architecture Schema and Flow

The architecture is composed of distinct layers: the **Frontend (Data Layer)**, the **Processing and Analysis Layer**, the **Data Storage (Database)**, and the **Service Layers (routes/API)**.


| Step                          | Layer/Component Involved          | Description                                                                                                                                                                                                                                                                 | Requirements Covered                                           |
|-------------------------------|-----------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------|
| **Artifact Discovering and Indexing** | Frontend (Data Layer)             | The process begins here, driven by user input (selecting directories/configuration). This layer is responsible for file walking, file type detection and categorization, metadata extraction (e.g., filename, type, creation date, size), and applying configurable directory rules. | Artifact Discovering and Indexing                             |
| **Analysis & Metric Generation**      | Processing and Analysis Layer     | Data moves here for deep processing. This layer performs analysis on various metrics, including lines of code per language, document word counts and complexity metrics, Git contribution analysis (e.g., counting commits, extracting dates). It also handles specialized processing like extracting image resolution and audio/video duration. Furthermore, complex functions like Duplicate Detection (using content hash comparison) and Project Grouping & Organization are integrated into the processing workflow. | Artifact Analysis, Duplicate Detection, and Project Grouping   |
| **Data Persistence**                  | Data Storage (Database)           | The metadata collected by the Frontend and the metrics generated by the Analysis Layer are stored. The database component supports the processing of said data. The system ensures data integrity using checksums before processing and adheres to security by storing all indexed data using local file encryption when at rest. Supabase tables include `uploads`, `projects`, and the new `resume_items` table that stores generated resume snippets per user with row-level security. | Artifact Discovering & Indexing, Security & Data Integrity, Resume Sync |
| **Data Retrieval & Service Provision**| Service Layers (routes/API)       | The API layer mediates access to the processed and stored data. This layer supports advanced functions like Search & Filtering capabilities based on criteria like filename, language, or date range.                                                                         | API Specification Design and Search & Filtering               |
| **Presentation & Export**             | Frontend (Data Layer), Service Layers | The frontend utilizes the service layers/APIs to display data. This includes generating Insights and Summaries (e.g., overall counts, timeline graphs based on creation or last edited dates). It also facilitates Export & Reporting functions, generating formatted reports (PDF, HTML) and data exports (JSON) for portfolio use. Users also have Data Privacy and Control functions at this stage, allowing them to exclude artifacts or delete indexed data. | Insight & Summaries, Export & Reporting, Data Privacy & Control |

### Optional LLM Media Analysis
An opt-in **LLMRemoteMediaAnalyzer** can send images/audio/video to an external LLM (GPT-4o family) for semantic summaries when explicitly enabled (e.g., `--llm-media`). Results are stored in `project_data.llm_media` without altering the local metadata pipeline. Usage: `python -m backend.src.cli.parse_zip <target> --json --llm-media` (or `./venv/bin/python -m â€¦`) to include remote summaries alongside local metadata.
