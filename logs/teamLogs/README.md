# Capstone Team 7 Logs

## Week 10 (November 3rd - 9th)
This week we focused on the implementing the local analyses into the CLI and moving to transfer into a textual based TUI. We also improved our external analysis, and added to our git analysis.

**Joaquin:** This week I kept pushing the TUI integration forward and refined some of the internal workflows so modules plug in more cleanly. I improved how the UI handles background tasks, cleaned up layout issues from last week, and synced the new analysis features with the updated scan pipeline. With that in place, I shifted most of my time toward reviewing teammates’ PRs to keep everything aligned as we merge toward a unified workflow.
For **[PR #123](https://github.com/COSC-499-W2025/capstone-project-team-7/pull/123)**, I confirmed the PDF workflow was solidly integrated into the CLI. The separation between parsing, analysis, and display was clean, the optional dependency handling was correct, and the tests and docs made the feature easy to understand and extend. For **[PR #124](https://github.com/COSC-499-W2025/capstone-project-team-7/pull/124)**, the implementation was simple and well-scoped, which will make integrating it into the TUI straightforward. Test coverage was good, and the logic for distinguishing individual vs collaborative projects was clear and reliable. For **[PR #125](https://github.com/COSC-499-W2025/capstone-project-team-7/pull/125)**, I really liked the completeness of the feature: code, CLI integration, tests, and docs were all aligned. The in-memory fallback was a nice touch for environments without Supabase. I suggested two changes: adding a note in the README that the CLI should only use the anon key, and double-checking `_get_authenticated_client` against the current supabase-py auth pattern to ensure the token is actually being attached to PostgREST requests. For **[PR #119](https://github.com/COSC-499-W2025/capstone-project-team-7/pull/119)**, the media analysis pipeline was well scoped with strong test coverage and clear README updates. I requested two fixes: remove heavy ML/audio dependencies from `requirements.txt` if the feature is optional, and guard the `.vision` import with a try/except so the scanner doesn’t break when dependencies aren’t installed. For **[PR #127](https://github.com/COSC-499-W2025/capstone-project-team-7/pull/127)**, the multi-project analysis and config flow were cleanly integrated, and the tests covered the new behavior thoroughly. No issues from my side.
Next week, I plan to continue merging these modules into the unified workflow and make sure the TUI presents them consistently. I also want to help refine any remaining UX friction as we move toward a smoother, end-to-end experience.


**Jacob:** 

I focused on delivering the torch-based media insights **[PR #119](https://github.com/COSC-499-W2025/capstone-project-team-7/issues/119#issue-3583123403) so images, video frames, and audio clips now get labeled, transcribed, and tagged with tempo/genre locally. I coordinated with Joaquin to make sure the CLI produces the data his Textual UI will consume once we wire it in, but the TUI integration hasn’t happened yet, so we’re handing him the metadata so he can plug it in next sprint.
Samarth and I synced on the LLM fallback design to keep the analyzer output schema aligned with the upcoming summarizer flow, and I reviewed Aaron’s Supabase schema PR to confirm how our media results will eventually be stored. Everyone’s work remains aligned even though the DB/TUI wiring is still pending.
Tested via python3 -m pytest tests/local_analysis/test_media_analyzer.py plus manual CLI runs over sample media folders; full-suite pytest still fails at tests/test_config_manager.py due to Supabase auth, which matches what the rest of the team sees. Next Cycle Plan (Milestone #1 prep) pair with Aaron to persist media analyzer outputs in Supabase once the shared schema lands (Milestone 1 storage requirement).
Help Samarth drop the LLM fallback into the CLI so our summaries remain reliable (Milestone 1 reliability requirement).
Coordinate with Joaquin to expose the new media insights inside the TUI as soon as his interface is ready to consume them (Milestone 1 UX requirement).

**Vlad:** 

**Aaron:** 

**Om:** This week delivered two major features that significantly improved user experience and system architecture: **PDF Analyser CLI Integration** and **Consent Persistence**. Both features were built upon and enhanced existing team infrastructure while creating reusable components for future development. I was also spending the rest of my team reviewing my team member's PRs to make sure code being pushed is clean and in accordance with standards. The focus for the team this week was to integrate their in house analysis work from last week into the existing CLI workflow created by Joaquin. Moving further, I had requested Samarth to work on designing the current LLM summarizer workflow so that we can have fallback models if OpenAI model reaches credit limits. Aaron decided to take on the responsibility to test and setup supabase table to be able to store data/information regarding the summary outputs and metadata for individual files (PDF to start with). Once he was able to implement the same, the team will make a decision if we need to go forward with tables for each type of file and store summary/metadata for each or keep one general project analysis table. As for Jake, he continued to research into better models (suggested to take a look at YOLO by ultralytics) for media analysis and implement the same to improve media file analysis. PR #122 was a great addition to the current CLI we have that allows our work to shine and makes it easier to test new features that get implemented in the backend, I was able to request changes to add better `error handling and visibility` for the users and `windows compatibility` for the bash scripts that were added. PR #120 was also a great addition as it added media analysis that Jake worked on previously to be integrated into the CLI workflow that we have. 

For the upcoming week, I hope to get some technical debt out of way with the help of my team. Further I plan to have a discussion on our path forward regarding the use of Supabase to store data and analysis outputs or not. Once this is decided upon everyone will start working hand in hand to incorporate their current in house analysis results into the database. With this being said, our final stretch will also include to start using these generated summaries and outputs to actually give specific insights on skills and metrics for the user's projects. Overall this was a very successful sprint in my opinion.

**Samarth:**


<p align="center">
  <img src="./charts/w10burnup.png" alt="Week 5 Burnup Chart width="400"/>
</p>

## Week 9 (October 27 - November 2)

This week we focused on the local analysis for multiple file types such as code files, media files, and documents beyond PDFs. We also worked on integrating past weeks' developments into a unified CLI workflow for various features.

**Joaquin:**
This week I focused on integrating all the separate modules into a single interactive CLI workflow. I coordinated with teammates to ensure every feature, including login, consent, scanning preferences, and analysis, worked smoothly together. The workflow is now fully connected to Supabase and supports session persistence for returning users. I tested everything through automated and manual runs.  I also reviewed multiple pull requests to support the new analysis modules. For **PR #107 (Local Document Analysis)**, I confirmed the implementation was clean, well-documented, and consistent with our existing architecture, noting that the expanded CLI and README updates were thorough and user-friendly. For **PR #113 (Enhanced Media Metadata Extraction and Reporting)**, I highlighted strong structure and testing, praised the clear separation between extraction, analysis, and display logic, and suggested pinning dependencies, clarifying the meaning of the `media_files_processed` count, confirming Python compatibility, and adding extra tests for missing or corrupt files. Next week I plan to implement the local analyses into the cli-workflow and work on improving user experience/interface

**Jacob:** Implemented an end‑to‑end media analysis pipeline so our project now surfaces actionable insights for images, audio, and video without calling the LLM: the scanner captures typed media_info, a deterministic MediaAnalyzer rolls those stats into summaries/issues, and a Rich/Questionary CLI mirrors the code analyzer UX so reviewers can explore the data interactively. These changes bring the backend closer to parity with our document analysis stack, giving us consistent local fallbacks and richer outputs for media‑heavy uploads. Next up I plan to expand the image side with deeper analysis (e.g., resolution quality checks, semantic labeling) so the insights go beyond metadata and align with what users expect from visual analysis.

**Vlad:** This week, I designed and implemented the Git parsing system that enables our application to analyze repository data both locally and from remote sources. The module extracts structured information such as commit history, contributor statistics, branch activity, and commit metadata (author, timestamp, message, and file change summaries). I integrated the parser into our existing backend so that it seamlessly interacts with the database and CLI workflows. To ensure robustness, I tested the system across repositories with different branching models and histories, confirming that it correctly handles merge commits, edge cases like detached HEAD states, and repositories with large commit volumes. I also added defensive error handling for missing .git directories and permission issues. Additionally, I contributed to the CLI integration, ensuring users can invoke Git parsing directly through a flag and receive clean, structured outputs. I worked with the team to align data formats between the Git parser, local analyzers, and Supabase ingestion layer so that future analytics and visualization modules can use a consistent schema. Next steps include extending the parser to capture PR-level data and code ownership metrics.

**Aaron:** This week, I developed and integrated a local code analysis system into our project scanning infrastructure. The implementation includes a comprehensive code parser module utilizing tree-sitter for multi-language support (Python, JavaScript, TypeScript, Java, C++, Go, Rust, and 7+ additional languages). The analyzer evaluates code quality through maintainability scoring that considers cyclomatic complexity, comment density, and function size metrics, automatically identifying files and functions that require refactoring attention. I built a companion interactive CLI that presents detailed analysis reports featuring security vulnerabilities, technical debt markers (TODOs/FIXMEs), complexity metrics, and prioritized refactoring recommendations. The analysis functionality was integrated into our existing parsing CLI through a new --analyze flag, allowing users to perform both project scanning and code quality assessment in a single operation. Throughout the implementation, I applied robust error handling patterns using Python's logging framework instead of print statements, and implemented defensive programming practices including safe attribute access, method validation, and graceful exception handling for incomplete analysis results. Documentation was added to explain the maintainability scoring algorithm, detailing how the 0-100 scale is computed through penalty-based adjustments for complexity, commenting practices, and function length. This feature provides teams with data-driven insights for technical debt management and code quality improvement across their projects.

**Om:** Implemented a multi-format local document analyzer supporting `.txt`, `.md`, `.markdown`, `.rst`, `.log`, and `.docx` files with comprehensive metadata extraction, markdown-specific features, automatic encoding detection, batch processing, and CLI tools. Integrated the existing PDF summarizer for consistent text analysis across all document types. Resolved roadblocks related to summarizer integration, encoding challenges, and paragraph count accuracy. The code for the document analyzer was merged after review of PR and I also worked on integrating the previously developed PDF summarizer into the CLI workflow for a unified experience across document types. Furthermore, worked with the team to decide on work distribution and making sure everyone is on track with their tasks. Also kept track of the overall progress and how the team is doing with respect to the timeline. Added reviews to almost each PR created by the team this week and spend some time coming up with work and task assignments for upcoming sprints.

**Samarth:** Integrated the LLM analysis module into the main application workflow by modifying client.py to add the summarize_scan_with_ai() orchestration method. Enhanced app.py with a complete AI analysis menu system (option 5), implementing session-level state management for API credentials and building _handle_ai_analysis() as the main handler with sequential validation checks. Created _render_ai_analysis_results() for Rich panel formatting and _export_ai_analysis() for markdown report generation. Implemented comprehensive privacy and security measures including no API key persistence, explicit external service consent, and automatic binary file filtering.

All the progress this week came together to make our project feel complete and connected. The new CLI workflow ties in everyone’s work, from login and scanning to analysis, into one smooth and interactive experience. The local analyzers for code, documents, and media now share a consistent structure, so everything runs reliably without relying on external calls. The Git parser adds useful repository insights, while the code and document analyzers make it easy to understand quality and content at a glance. On top of that, the AI module brings an intelligent layer that summarizes results securely and clearly.


<p align="center">
  <img src="./charts/w9burnup.png" alt="Week 5 Burnup Chart width="400"/>
</p>

## Week 8 (October 20 - 26)
This week we continued developing the backend, moving from setup into more functional implementation. Alongside expanding the CLI to better support backend workflows.

**Joaquin:**  This week I improved the CLI’s parsing features. I added safer ZIP handling that skips unnecessary folders, made the table display reusable, and introduced two new flags: `--relevant-only` (to include only key project files) and `--code` (to show language breakdowns). The `README` now includes examples, and all new features are tested and verified to work. I also focused heavily on reviewing and providing feedback for several major pull requests to help align backend structure and ensure consistent design across modules. I reviewed **PR #89 (Consent Module Integration)**, **PR #95 (Database Config Manager)**, **PR #96 (Local PDF Analysis)**, **PR #97 (Supabase Auth and Consent CLI)**, and **PR #98 (LLM Analysis System)**. My feedback covered structure, security, and maintainability, such as suggesting token redaction in the auth CLI, flagging duplicate methods in the config manager, recommending import-safe naming in the local analysis module, and improving client initialization and logging in the LLM system.  Collaboration went well, with everyone responsive to feedback and quick to make revisions. These reviews helped strengthen consistency and security across the backend. Next week, I will focus on connecting the parsing improvements to the interactive CLI workflow and linking them with Supabase login and user scanning preferences.

**Jacob:** This week I worked on the backend by starting the Supabase integration. I set up the environment with the project URL and anon key, created the initial database schema, and began defining storage policies. I also started on an upload test to check file and metadata handling, but ran into bugs that I’m still working through. While I didn’t get to a finished PR yet, this lays the base for connecting secure storage to the CLI in the next step.

**Vlad:** Focused on integrating Supabase authentication and consent management into the backend CLI as part of issue #86. Extended auth_cli.py to support secure sign-up, log-in, and access-token retrieval directly from the terminal, allowing verified users to authenticate and submit consent records to the Supabase database. Added a new SQL migration file, 04_consent_policies.sql, to define row level security (RLS) policies ensuring that each user can only access or modify their own consent data. Tested the complete CLI workflow end to end, including token handling, database persistence, and error cases and confirmed seamless interaction between the authentication layer and the Consent Validation Module. 

**Aaron:** This week I refactored the config manager class to work with the database. This involved ensuring crud operations work with the supabase db we setup. I also added testing for all use cases. I then began the local analysis for coding feature, but yet to make a PR for that. The branch has the beginning implementation now for analyzing files for coding metrics. I made a PR for the refactored config manager that uses the db now, this includes new methods like 'get_allowed_extensions()' and changing code so that we store to the database.

**Om:** Implemented a privacy‑first local PDF analysis pipeline including a robust PDFParser that extracts text and metadata with configurable size and page limits, and a PDFSummarizer that uses an in‑house TF‑IDF extractive approach with sentence filtering, tokenization, keyword extraction, and document statistics. I added a user-friendly CLI `(pdf_cli.py)` plus quick-reference docs to parse, summarize, batch-process, and inspect PDFs. The test suite was expanded to 68 tests covering edge cases and integration paths, yielding 100% coverage for the summarizer and high coverage for the parser. I migrated from the deprecated PyPDF2 to pypdf, added a conftest import helper for cleaner tests, and exposed factory functions `(create_parser, create_summarizer)`. For Supabase consent flows I hardened `auth_cli.py` to prompt securely for passwords, added revoke/delete support, and updated SQL with an RLS DELETE policy for safe consent revocation. README and CLI references were updated for usage and CI-friendly testing, and added minor docs and examples.

**Samarth:** Worked on developing the LLM-powered summarization and analysis module for the system, generating structured insights from portfolio data. Implemented the `summarize_tagged_file()` and `analyze_project()` functions to produce detailed summaries, technical highlights, and qualitative analysis for resume-ready reports. Also built helper functions including `chunk_and_summarize()` for efficient large-file handling, `_count_tokens()` for dynamic token measurement, and `_make_llm_call()` for standardized LLM communication. Finally, added `suggest_feedback()` to deliver personalized career aligned insights. 

All the updates this week build on each other to make the whole system smarter and more reliable. The new CLI features make it easier to parse and display project data, while the authentication and consent setup ensures everything stays secure and user-specific. The local PDF tools keep things privacy-first but still powerful enough for deep analysis, and the LLM module ties it all together by turning that data into useful insights. Altogether, these improvements connect the technical, privacy, and intelligence sides of the system so it runs more efficiently and feels more seamless to use.


<p align="center">
  <img src="./charts/w8burnup.png" alt="Week 5 Burnup Chart width="400"/>
</p>


## Week 7 (October 13 - 19)

This week was our first real dive into backend development. After spending the past few weeks planning and documenting, we finally started building the core of the system: setting up the main functions, initializing the database, adding user consent handling, adding configuration profiles, and creating the first version of the file parsing pipeline. The main goal was to lay a solid foundation so future milestones can build on a working backend

**Joaquin:** Focused on building the archive ingestion pipeline in `backend/src/scanner/parser.py`. Implemented path validation, zip handling, and traversal protection, while structuring results through `FileMetadata`, `ParseIssue`, and `ParseResult` dataclasses. Added clear error types (`UnsupportedArchiveError`, `CorruptArchiveError`) and a CLI tool in `scripts/parse_archive.py` for testing. Updated the `README.md` with setup and usage instructions for the new parser.

**Jacob:** 
Implemented the Consent Management Module responsible for handling user permissions when interacting with external services such as LLMs. Developed core functions to request, record, verify, and withdraw user consent, integrating a detailed privacy notice to inform users about data transmission and storage risks prior to granting permission. Added a comprehensive unit test suite with 5 test cases covering positive and negative consent flows, default states, and withdrawal handling. Resolved rebase conflicts with main to ensure seamless backend integration and submitted a structured pull request documenting all changes.

**Vlad:** 
Setup of the project’s Supabase backend infrastructure, including database initialization, secure row-level storage policies, and integration testing. Created SQL scripts to define and automate key database components such as the profiles and uploads tables, triggers for user creation in auth.users, and row-level security (RLS) policies ensuring users can only access their own storage objects. Implemented an end-to-end upload test (test_upload.mjs) to verify database and Supabase Storage integration. 

**Aaron:** Implemented the configuration scanning profile logic by adding the 'ConfigManager' class which has various methods allowing the user to add, remove, switch between, and delete scanning profiles based on their own preferences in regard to file extensions. Currently stores the users config scanning profile in a json file, but will change to storing in database this week via Supabase. Also added a test suite with 18 test cases to cover all scenarios the user will encounter when utilizing any method pertaining to the configuration profile to ensure robust functionality.  

**Om:** Designed and implemented the Consent Validation Module with comprehensive `ConsentValidator` class, custom exception handling, and `ConsentRecord` dataclass. Developed core validation methods for upload consent, external services, and permission checks. Created extensive unit test suite with 20+ test cases, fixtures, and integration scenarios to ensure robust privacy-compliant functionality.

**Samarth:** Focused on building the LLM Integration setup enabling external AI service capabilities with secure API key verification and robust error handling. Developed the `LLMClient` class for OpenAI API integration, with complete configuration management and consent-based access control. Implemented RESTful API routes for key verification, model information, and service status. Built a comprehensive suite of 30+ unit and integration tests covering client initialization, authentication workflows, and endpoint behavior to ensure secure and reliable LLM operations.

<p align="center">
  <img src="./charts/w7burnup.png" alt="Week 5 Burnup Chart width="400"/>
</p>

## Week 6 (October 6 - 12)

This week our focus was on getting our project setup in place and updating some of our main documents. We started by creating a GitHub Project to organize all our tasks and make progress tracking easier with its built-in burnup chart. 

All previous tasks and the new Milestone 1 requirements were added as issues, each labeled by category and assigned story points so the chart reflects effort more accurately. After that, we worked on updating our Level 1 DFD. Some of the key changes included adding a consent gate, splitting the analysis process into local and external parts, keeping the user more involved through the UI loop, and showing more detail on how data moves into the database. We also cleaned up the process descriptions and arrows so the flow is easier to follow. We also updated our System Architecture diagram to reflect the changes brought by the new requirements, specifically the disctintion between the local and external analysis options. The Work Breakdown Structure was also expanded to cover all the specific tasks and deliverables from Milestone 1, before, it was more general and based on our early understanding of the requirements. Morevoer, the repo was setup with all of our initialy directories so we can start working on our backend. FInally, A Dockerfile was also added to standardize the environment setupand the README was updated to match the current directory structure and now includes direct links to our main documentation: Work Breakdown Structure, Data flow diagrams, and System Architecture.

<p align="center">
  <img src="./charts/w6burnup.png" alt="Week 6 Burnup Chart width="400"/>
</p>

## Week 5 (September 29 - October 5)

This week our focus was on the Data Flow Diagrams (Level 0 and 1).

We started off the week with listing down some simple processes from the start to the end in our google document. This process then led to us discovering some other interconnected processes which allowed us to narrow down onto the 7 main processes that would control the entire flow of data in our diagram. From here we had to just draw the shapes for each one of them and add appropriate description. The next steps were collective efforts into deciding the process flow directions for the different processes and their inputs and outputs. The end step was to add into picture the data storage aspect and connect it to the rest of the diagram. The shapes were then adjusted to match the notation from lecture so the diagram looked clear and consistent, and copies were printed to share with other groups. When comparing diagrams, it became clear that some groups had missing or inconsistent data stores, which made their flows harder to follow and less organized. We also checked over our own diagram to make sure the data stores were being reused correctly across processes. Finally, the repo was reorganized by moving the logs directory out of the docs folder and into its correct place, making the structure consistent with class practices.

<p align="center">
  <img src="./charts/w5burnup.png" alt="Week 5 Burnup Chart width="400"/>
</p>


## Week 4 (September 22 - 28)

This week we focused on the system architecture and the project proposal.

For the architecture, we first made a detailed component diagram that broke down each layer and described the components inside them. While it helped us see exactly what pieces exist in the system, the problem was that the flow of information wasn’t obvious, the arrows just went from one layer to the next without showing how data would actually move. After discussing, we made a second diagram that was less detailed but much clearer in terms of flow. The first diagram works well for showing system structure, while the second works better for understanding process flow. Together they allow for a pretty good understanding of the system.

We also finished the project proposal, which included:
- Usage scenario (Samarth)  
- Proposed solution (Vlad)  
- Use cases (Joaquin & Jacob) – covering artifact discovery, analysis, privacy, reporting, search/filter, etc.  
- Requirements & testing (Om & Aaron) – both functional and non-functional, linked to test frameworks (Jest, Playwright, etc.), with difficulty levels assigned.  

## Week 3 (September 15 - 21)

We worked on developing ideas for the functional and non-functional requirements for the Project Requirements document. Additionally, we added information regarding the target user group and usage scenarios. We also spent time discussing the requirements in class and learning about other teams' requirements as well. One thing we noticed we did not do that other teams did was define a tech stack, but we think it would be better to define our tech stack once we have more defined project specifications.


